# 내일배움캠프 16일차 TIL


# Week5
## 오토인코더
(Autoencoder) : 데이터의 중요한 특징을 학습하고 이를 복원하는 비지도 학습
차원축소, 잡음제거, 생성 모델 등
* 구조
  * 인코더 : 인풋을 저차원 공간으로 표현( 중요 특징만 추출 )
    * 잠재 공간 (Latent Space) : 높은 차원의 입력 데이터를 저차원으로 바꾼 공간 (공간이 압축됨)
  * 디코더 : 추출된 정보를 바탕으로 원본 데이터를 만듬
* 종류
  * 딥 오토인코더 : 딥러닝에서의 오토인코더
  * 변분 오토인코더 (VAE) : 데이터의 잠재 공간을 사용해 데이터 분포 학습
  * 희소 오토인코더 : 출력이 희소하게 하여 중요 특징만 학습
  * 잡음 제거 오토인코더 : 입력 데이터에 잡음 추가, 이를 제거하는 학습을 통해 데이터 복원 능력 향상


## 생성형 모델 학습
### GAN
* 구성요소
  * 생성자 : 랜덤 데이터를 받아 실제 이미지처럼 가짜 이미지를 만들어내도록 학습
  * 판별자 : 생성된 이미지가 진짜인지 가짜인지 판별
* 핵심 원리
  * 두 개의 모듈이 서로 경쟁 하며 학습
    * 생성자는 판별자를 속이기 위해 가짜이미지를 만들고, 판별자는 진위여부를 잘 판단하기위해 발전함
* 장점
  * 고품질 데이터 생성
  * 다양한 작업 가능
* 단점
  * 훈련이 불안정함 : 경쟁이 잘 조율되게 만들기 쉽지 않음


## VAE
(Variational Autoencoder)
입력 데이터를 잠재 공간에 확률 분포로 모델링 해서 새로운 데이터 생성 후 해당 데이터의 구조적 특징 학습하는 모델
* 구조
  * 인코더 : 입력 데이터를 잠재 변수(확률분포를 따름)로 변환
  * 디코더 : 잠재 변수로 부터 원래 데이터를 복원
  * 손실함수
    * 리컨트럭션 로스 : 원래 데이터와 복원된 데이터간의 차이를 최소화
    * 쿨백라이블러 발산? : 인코더가 학습한 잠재 분포와 정규분포간의 차이를 최소화
* 장점 
  * 새로운 데이터를 생성 
* 단점
  * 복잡해서 훈련이 어려움

## 전이 학습
(Transfer Learning)
이미 학습된 모델의 지식을 새로운 문제에 적용해 새로운 문제를 더 빠르고 효과적으로 해결
* 장점
  * 데이터 부족할 때 기존 모델의 지식을 활용해 해결 가능
  * 사전 학습된 모델 사용시 **학습 시간 단축**됨
  * 성능 향상
* 원리
  * 특징 추출기 : 사전 학습된 모델의 초기 층 고정, 새 데이터에 맞게 **마지막 층만 재학습**
  * 미세 조정 (Fine-Tuning) : 사전 학습 **모델 전체**를 새로운 데이터에 맞게 재학습(가중치 바꾸기)
* 과정
  * 사전 학습 모델 로드 : PyTorch 에서 제공하는 사전 학습된 모델 로드
  * 모델 수정 : 사전 학습된 모델의 마지막 층을 새 문제에 맞게 수정
  * 모델 학습 : 수정된 모델 새 데이터에 맞게 학습


---

# Week6
## 과적합 방지 기법
과적합 : 모델이 학습데이터에 지나치게 의존해서 일반화가 안되는 문제
### 정규화
(Normalization) : 데이터를 일정 범위로 조정
* 종류
  * 배치 정규화 : 미니배치에서 특징의 평균과 분산을 사용해 활성화 값을 정규화->학습 속도 향상. 초기화 값에 민감하지 않아도 됨. 과적합 방지
    * 미니배치 : 전체 데이터셋을 작은 부분 집합으로 나눈 것
    * 활성화 값 : 활성화 함수를 통해 계산됨
  * 레이어 정규화 : 각 레이어의 활성화 값을 정규화. 배치크기에 영향을 받지 않고 시퀀스 데이터에 적합해 RNN에 주로 쓰임
  * 그룹 정규화 : 채널을 그룹으로 나누고 각 그룹에서 정규화
  * 인스턴스 정규화 : 각 샘플에 대해 정규화

### 드롭아웃
학습 과정에서 무작위로 뉴런을 비활성화해 과적합 방지
* **학습 시에만 적용!!!** 평가 시에는 모든 뉴런을 활성화 해야 함

### 조기 종료
* 검증 데이터의 성능이 더 이상 향상되지 않을 때 학습 중단

### 데이터 증강
원본 데이터를 변형해 새 데이터 생성하여 데이터셋 확장해 모델의 일반화 성능 향상

## 하이퍼파라미터 튜닝
최적화 방법은 모델의 가중치를 업데이트 해서 손실함수를 최소화 하는 알고리즘임
* 용어
  * 학습률
    * 어느정도의 비율로 학습할 지 정함
    * 너무 큼- > 학습 불안정 / 너무 작음 -> 학습 느려짐 
    * 일반적으로 0.1,0.01,0.001 등의 값 사용
  * 배치 크기
    * 한 번의 업데이트에 사용되는 데이터의 샘플 수 결정
    * 클수록 학습 속도는 증가하지만 메모리 사용량도 증가
    * 일반적으로 32,64,128 
  * 에포크 수
    * 전체 데이터셋 학습 반복 횟수
    * 너무 적으면 과소적합, 너무 크면 과적합 발생
    * 조기 종료 기법을 사용해 적절한 에포크 수를 결정 가능
  * 모멘텀
    * 경사하강법 : 손실함수를 최소화하기 위해 모델의 가중치를 반복적 업데이트
    * 이전 기울기를 현재 기울기에 반영해, 학습 속도를 높이고 진동을 줄임
    * 일반적으로 0.9, 0.99 값
  * 가중치 초기화
    * 모델의 가중치를 초기화 하는 법 결정

---

## 파이썬 코드카타 DAY 8
```py
# 배열 자르기
def solution(numbers, num1, num2):
    return numbers[num1:num2+1]

# 외계행성의 나이
def solution(age):
    a=['a','b','c','d','e','f','g','h','i','j','k']
    th = age // 1000
    h = (age // 100) % 10
    t = (age // 10) % 10
    o = age % 10
    if th>0:
        answer=''.join((a[th],a[h],a[t],a[o]))
    elif h>0:
        answer=''.join((a[h],a[t],a[o]))
    elif t>0:
        answer=''.join((a[t],a[o]))
    else:
        answer=''.join(a[o])
    return answer


# 진료 순서 정하기
def solution(emergency):
    answer1 = []
    answer2 = []
    
    answer2 = sorted(emergency,reverse=True)
    
    for i in emergency:
        answer1.append(answer2.index(i)+1)
    return answer1


# 순서쌍의 개수
def solution(n):
    count=0
    for i in range(1,n+1):
        if n%i==0:
            count+=1
    return count

```