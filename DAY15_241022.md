# 내일배움캠프 15일차 TIL
## 딥러닝 : Week3
### 어텐션 (Attention) 메커니즘
시퀀스 데이터(문자열 등)에서 중요한 부분에 많은 가중치를 할당해 정보를 효율적으로 처리
* 동작 방식
  * 입력 시퀀스를 보고 중요도(Attention Score)를 계산 후 가중치로 부여함
    * 세 가지 구성요소
      * Query
      * Key
      * Value
    * Attention Score 계산 : Query 와 Key 간의 유사도를 측정한 후 dot product 를 통해 계산
    * Softmax 함수를 통해 얻어진 가중치를 Value에 곱해 최종 Attention에 출력 계산
* 결과 : 시퀀스 데이터가 이해하기 쉬운 형태로 변환되어 모델에 적용하면 좋은 성능을 발휘함
* 종류
  * Self Attention : 위의 동작방식
  * Multi-Head Attention : 여러 개의 Self Attention을 병렬로 수행하며 각 Head가 서로 다른 부분을 학습하여 모델이 다양한 관점을 가짐

### 자연어 처리 (NLP) 모델
컴퓨터가 사람의 언어를 이해하고 처리하는데 사용되는 기계 학습 모델 전체를 의미
* 워드 임베딩 (Word Embedding) : 단어를 고정된 크기의 벡터(숫자)로 변환
  * Word2Vec : 단어를 벡터로 변환하는 두 가지 모델 제공
    * CBOW : 주변 단어를 보고 중심단어 예측
    * Skip-gram : 중심 단어를 중심으로 주변 단어 예측
  * GloVe : 단어-단어 공기행렬을 사용해 단어 벡터 학습. 이를통해 단어를 벡터로 바꿈
* 시퀀스 모델링 (Sequence Modeling) : 순차적인 데이터를 처리하고 예측하는 모델링 기법
  * 입력 시퀀스
  * 은닉 상태
  * 출력 시퀀스
* Transformer 와 BERT : 순차적 데이터를 병렬로 처리할 수 있는 모델. 자연어 처리에서 뛰어난 성능
  * Transformer : 인코더-디코더 구조
    * 인코더 : 셀프 어텐션(문장 내 관계 학습), 피드포워드 신경망
      * 입력 단어 벡터들이 다른 단어와의 관계를 반영한 새로운 백터로 만들어짐
    * 디코더 : 셀프어텐션, 인코더 디코더 어텐션(입력과 출력 연결), 피드포워드 신경망
      * 타겟 문장의 각 단어를 고정된 크기의 벡터로 변환. 포지셔널 인코딩 추가
    * BERT : Transformer 인코더를 기반으로 한 사전 학습된 언어 모델
      * 파인튜닝 할 시 다양한 목적에 맞춰 사용 할 수 있음
---
## 딥러닝 : Week4
비전모델 : 대부분 CNN 을 기반으로, 새로운 기법을 도입하여 성능을 좋게만듬
### ResNet
* 딥러닝
  * 딥러닝에서 발생했던 문제점 : 계층을 많이 쌓을수록 입력데이터가 변형되어 끝에선 입력데이터의 특징을 거의 반영하지 못하는 현상 -> ResNet 으로 해결 
* 깊은 신경망을 학습하기 위해 개발된 모델
* 잔차 학습(Residual Learning) : 네트워크가 직접 입력을 학습하는 대신 잔차(입력과 출력의 차이)를 학습하게 함 -> 네트워크의 깊이를 깊게 늘려 성능 향상
* ResNet 모델의 숫자는 각 네트워크의 층수를 나타냄
* 특징 
  * 잔차 학습 -> 기울기 소실 문제 해결
  * 간단한 블록 구조 -> 네트워크 쉽게 확장
  * 딥러닝 네트워크의 깊이를 늘릴 수 있게 함 -> 다양한 분야에서 높은 성능
* 깊은 층 네트워크 생성시, 기울기 소실문제로 인해 학습이 잘 안되는 경우 ResNet의 잔차학습 개념을 도입하면 성능을 개선할 수 있음

### 이미지 처리 모델
* ResNet
* VGG
  * ResNet 이전에 깊은 층을 이용해 성능을 올린 모델. 16 또는 19 개의 층을 가짐. CNN에서 작은 필터를 사용해 깊이를 증가시킴. 깊어서 계산비용 높음
* Inception
  * 인셉션블록 : 다양한 크기의 필터를 동시 사용해 입력 데이터로부터 특징 추출. 다양한 특징을 한번에 추출. 계산량을 줄여 효율적. 복잡하지만 얕은 구조
* YOLO (You Only Look Once) 
  * 객체 탐지 모델
  * 이미지에서 객체의 위치와 클래스를 동시 예측
  * 전역적임 : 이미지 전체를 한 번에 분석과 처리. 빠르고 정확함
  * 효율적 : 실시간으로 객체 탐지. 단일 신경망 모델. 객체 탐지와 분류 동시
* 이미지 세그멘테이션 : 이미지를 픽셀 단위로 분해 -> 각 픽셀이 어떤 객체나 영역에 속하는지 분류
  * 시맨틱 세그맨테이션 : 이미지의 각 픽셀을 클래스 레이블로 분류 (단, 같은 클래스는 내부적으로 따로 구분 하지 않음)
  * 인스턴스 세그멘테이션 : 같은 클래스 내에서도 객체 구분 (예 : 사람)
  * 주요 세그멘테이션 모델
    * FCN : 픽셀 단위의 분류 가능 - 시멘틱 세그멘테이션
    * U-Net : FCN 의 발전된 모델 , 인코더-디코더. 소량의 데이터에서 높은 성능( 의료 이미지 )
    * Mask R-CNN : 다양한 어플리케이션에 적용됨

## 파이썬 코드카타 DAY 7
```py
# 특정 문자 제거하기
def solution(my_string, letter):
    return my_string.replace(letter,"") # replace(대체하려는기존문자열, 대체할내용) 메서드로 바꿀 값 없애기

# 각도기
def solution(angle):
    if (0<angle)&(angle<90):
        answer =1 
    elif angle==90:
        answer=2
    elif (90<angle)&(angle<180):
        answer=3
    else:
        answer=4
    return answer

# 양꼬치
def solution(n, k):
    y=12000 # 양꼬치 가격
    d=2000 # 음료 가격
    free_drink=n//10 # 10인분마다 제공되는 음료 개수
    total_y=n*y # 양꼬치 총 가격
    total_d=(k-free_drink)*d # 음료 총 가격(마시는음료수-제공되는음료개수)
    return total_y+total_d

# 짝수의 합
def solution(n):
    i=2 
    answer=0
    while i<=n:
        if i%2==0: # i 가 짝수이면
            answer+=i # 답변에 짝수 더하기
        i+=1 # i 증가시키기
    return answer
```
---
## 추가 공부
* replace 메서드 : `string.replace(기존문자열,대체내용,개수)`
  * 기존 문자열 : 대체하려는 기존의 문자열
  * 대체 내용 : 새로운 문자열로 대체할 내용
  * 개수 : 대체할 최대 횟수. 지정하지 않을 시 모두 대체