# Week5 : 앙상블학습
- 앙상블 학습 (Ensembel Learning)
    - 여러 개의 학습 모델을 결합해 하나의 모델을 만듬
    - 단일 모델보다 더 높은 예측 성능과 일반화 능력을 얻음
  
## 배깅과 부스팅

### 배깅 
(Bootstrap Aggregating)
* 여러 개의 학습 모델 병렬 학습
* 데이터 샘플링 과정에서 Bootstrap 기법을 사용해 원본 데이터셋에서 중복 허용한 무작위 샘플 생성
* 장점
  * 과적합 감소
  * 안정성 향상
  * 병렬 처리 가능 : 여러 개의 학습 모델을 병렬로 학습시킴


### 부스팅
(Boosting)
* 여러 개의 약한 학습기를 순차척으로 학습 → 예측 결과를 결합해 강한 학습기로 만듬
* 이전 모델이 잘못 예측한 데이터 포인트에 가중치를 부여해 다음 모델 보완
* 장점
  * 높은 예측 성능
  * 과적합 방지
  * 순차적 학습

## 랜덤 포레스트
(Random Forest)
: 의사결정 트리를 앙상블학습으로 개선한 것
* 배깅 기법을 기반으로 한 앙상블 학습 모델
* 여러 개의 결정트리를 학습시킴, 그 예측 결과를 결합해 최종 예측 수행
* 장점 : 각 트리 독립적 학습
  - 과적합 방지
  - 다형성 증가
  - 높은 예측 정확성
  - 변수에 중요도 평가를 할 수 있음
  - 해석이 쉬운 모델


## 그래디언트 부스팅 머신 (GBM)
: 랜덤 포레스트 모델 대신 사용할 수 있음
* 결정 트리를 부스팅 기법으로 만들어낸 모델
* 기존 모델이 만든 오차를 다음모델이 개선하도록 미분 사용



## **XGBoost** : 성능 GOOD!!!!
 (eXtreme Gradient Boosting)
 : 그래디언트 부스팅 알고리즘을 기반으로 한 고성능 앙상블 학습 기법
 * 최적화 단계가 이미 포함됨
 * 장점
   * 병렬처리 : 학습 속도 향상
    * 조기 종료 : 데이터셋 성능이 향상되지 않을 때 학습 조기 종료 가능. 과적합 방지
    * 정규화 : 과적합 방지
    * 유연성