# Week3
## 어텐션 (Attention) 메커니즘
시퀀스 데이터(문자열 등)에서 중요한 부분에 많은 가중치를 할당해 정보를 효율적으로 처리
* 동작 방식
  * 입력 시퀀스를 보고 중요도(Attention Score)를 계산 후 가중치로 부여함
    * 세 가지 구성요소
      * Query
      * Key
      * Value
    * Attention Score 계산 : Query 와 Key 간의 유사도를 측정한 후 dot product 를 통해 계산
    * Softmax 함수를 통해 얻어진 가중치를 Value에 곱해 최종 Attention에 출력 계산
* 결과 : 시퀀스 데이터가 이해하기 쉬운 형태로 변환되어 모델에 적용하면 좋은 성능을 발휘함
* 종류
  * Self Attention : 위의 동작방식
  * Multi-Head Attention : 여러 개의 Self Attention을 병렬로 수행하며 각 Head가 서로 다른 부분을 학습하여 모델이 다양한 관점을 가짐

## 자연어 처리 (NLP) 모델
컴퓨터가 사람의 언어를 이해하고 처리하는데 사용되는 기계 학습 모델 전체를 의미
* 워드 임베딩 (Word Embedding) : 단어를 고정된 크기의 벡터(숫자)로 변환
  * Word2Vec : 단어를 벡터로 변환하는 두 가지 모델 제공
    * CBOW : 주변 단어를 보고 중심단어 예측
    * Skip-gram : 중심 단어를 중심으로 주변 단어 예측
  * GloVe : 단어-단어 공기행렬을 사용해 단어 벡터 학습. 이를통해 단어를 벡터로 바꿈
* 시퀀스 모델링 (Sequence Modeling) : 순차적인 데이터를 처리하고 예측하는 모델링 기법
  * 입력 시퀀스
  * 은닉 상태
  * 출력 시퀀스
* Transformer 와 BERT : 순차적 데이터를 병렬로 처리할 수 있는 모델. 자연어 처리에서 뛰어난 성능
  * Transformer : 인코더-디코더 구조
    * 인코더 : 셀프 어텐션(문장 내 관계 학습), 피드포워드 신경망
      * 입력 단어 벡터들이 다른 단어와의 관계를 반영한 새로운 백터로 만들어짐
    * 디코더 : 셀프어텐션, 인코더 디코더 어텐션(입력과 출력 연결), 피드포워드 신경망
      * 타겟 문장의 각 단어를 고정된 크기의 벡터로 변환. 포지셔널 인코딩 추가
    * BERT : Transformer 인코더를 기반으로 한 사전 학습된 언어 모델
      * 파인튜닝 할 시 다양한 목적에 맞춰 사용 할 수 있음