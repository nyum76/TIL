{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AI_8/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"I have a cat, and since my cat isn't aggressive, how can she keep a cat off the leash? I was not about to make her go out with someone, and I have to tell her and make her aware of the fact that the\"}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt-2 모델\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# 파이프라인 로드\n",
    "generator = pipeline(\"text-generation\", model = \"gpt2\")\n",
    "\n",
    "#텍스트 생성\n",
    "result = generator(\"I have a cat\", max_length=50,num_return_sequences=1)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AI_8/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I have a cat, I mean the cat is sitting between my cat and myself. I have no one else in the house to do the housework so they need to bring them there and sit up there at night,\" he says. \"We all'},\n",
       " {'generated_text': \"I have a cat now, I've got a cat now but will she want to get into the shower? The doctor says your cat will not give food to her. If they put a cat down in the shower, she will want to go out\"}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# gpt-2 모델\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# 파이프라인 로드\n",
    "generator = pipeline(\"text-generation\", model = \"gpt2\")\n",
    "\n",
    "#텍스트 생성 : num_return_sequences 변경 -> 문장 2개 생성\n",
    "result = generator(\"I have a cat\", max_length=50,num_return_sequences=2)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"I have a cat named Daphne who loves to eat chocolate, and she's become this wonderful pet. She was really lovely but I couldn't get her to stop crying. It wasn't long before I was told she didn't give a damn. I had no idea her behaviour at that point until she had told me about her cat's cruelty. After some research she'd finally come to my attention and I asked the couple if they'd like to do another pet intervention! Apparently it'd always\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt-2 모델\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# 파이프라인 로드\n",
    "generator = pipeline(\"text-generation\", model = \"gpt2\")\n",
    "\n",
    "#텍스트 생성 : 최대 길이 변화 - 항상 최대길이를 다 채우지는 않는 모습\n",
    "result = generator(\"I have a cat\", max_length=100,num_return_sequences=1)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/opt/anaconda3/envs/AI_8/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998656511306763}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 감정 분석 (Sentiment Analysis)\n",
    "from transformers import pipeline\n",
    "\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "result = sentiment_analysis(\"I love you\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AI_8/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e5f95244be41849741a88f25261e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f07402c83a4ccba1e613d0a5c161e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1e8b66b32b4c76bbbee85be9bb5c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7d2ca9be7d4b30a9e9c25a83d5b5ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d38f6692c4042f0908b14dd0bc7545e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7262282668f14d36b85ac5ca4992cf9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.5302404165267944}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 감정 분석 (Sentiment Analysis)\n",
    "from transformers import pipeline\n",
    "\n",
    "# bert 사용\n",
    "# 이랍ㄴ적인 언어 모델링 작업에 대해 사전 학습된 모델이지만, 특정 작업에 바로 사용할 수 있는 가중치가 없기에 파인튜닝이 필요함\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model = \"roberta-base\")\n",
    "result = sentiment_analysis(\"I love you\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'quick', 'brown', 'fox', 'joumps', 'over', 'the', 'lazy', 'dog'],\n",
       " ['love', 'playing', 'with', 'my', 'pet', 'dog'],\n",
       " ['the', 'dog', 'barks', 'at', 'the', 'stranger'],\n",
       " ['the', 'cat', 'sleeps', 'on', 'the', 'sofa']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word2Vec 사용하기 전 준비\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from scipy.spatial.distance import cosine # 코싸인 유사도 활용\n",
    "\n",
    "sentences =[\n",
    "    \"The quick brown fox joumps over the lazy dog\",\n",
    "    \"I love playing with my pet dog\",\n",
    "    \"The dog barks at the stranger\",\n",
    "    \"The cat sleeps on the sofa\"\n",
    "]\n",
    "\n",
    "processed = [simple_preprocess(sentence) for sentence in sentences] # 단어 단위 문장 쪼개기\n",
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.16885965776110257"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word2Vec 모델 생성\n",
    "model = Word2Vec(sentences = processed, \n",
    "                 vector_size = 5, #  각 단어를 몇차원 공간에 저장할지\n",
    "                 window = 5, # 주변 단어 범위\n",
    "                 min_count = 1, # 임베딩할 단어(학습에 포함시킬 단어)의 최소 등장 수\n",
    "                 sg = 0 # Word2Vec 이 어떤 알고리즘을 통해 학습할지\n",
    ")\n",
    "\n",
    "dog = model.wv['dog']\n",
    "cat = model.wv['cat']\n",
    "\n",
    "sim = 1 - cosine(dog,cat)\n",
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the two sentences: 0.5665\n"
     ]
    }
   ],
   "source": [
    "# Bert 기반의 임베딩 모델\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name) # 토크나이저\n",
    "# 모델 생성\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# 임베딩할 문장 가져오기\n",
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"minmin is such a cutie\"\n",
    "]\n",
    "\n",
    "input1 = tokenizer(sentences[0], return_tensors='pt') \n",
    "input2 = tokenizer(sentences[1], return_tensors='pt')\n",
    "\n",
    "with torch.no_grad(): # 연산 속도를 올리고 메모리 사용을 줄이기 위해 학습단계에서 필요한 오토그래드를 끄는 부분\n",
    "    output1 = model(**input1)\n",
    "    output2 = model(**input2)\n",
    "\n",
    "# 1차원으로 만들어주기 : 코싸인 계산을 가능하게 하도록 임베딩 형태에 따라 차원을 변환\n",
    "embedding1 = output1.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "embedding2 = output2.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "similarity = 1 - cosine(embedding1, embedding2)\n",
    "\n",
    "print(f\"Cosine similarity between the two sentences: {similarity:.4f}\") # 유사도 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the two sentences: 0.6565\n"
     ]
    }
   ],
   "source": [
    "# Bert 기반의 임베딩 모델\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name) # 토크나이저\n",
    "# 모델 생성\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# 임베딩할 문장 가져오기\n",
    "sentences = [\n",
    "    \"minmin is such a cutie\",\n",
    "    \"puppy is really cute, and he looks like puppy\"\n",
    "]\n",
    "\n",
    "input1 = tokenizer(sentences[0], return_tensors='pt') \n",
    "input2 = tokenizer(sentences[1], return_tensors='pt')\n",
    "\n",
    "with torch.no_grad(): # 연산 속도를 올리고 메모리 사용을 줄이기 위해 학습단계에서 필요한 오토그래드를 끄는 부분\n",
    "    output1 = model(**input1)\n",
    "    output2 = model(**input2)\n",
    "\n",
    "# 1차원으로 만들어주기 : 코싸인 계산을 가능하게 하도록 임베딩 형태에 따라 차원을 변환\n",
    "embedding1 = output1.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "embedding2 = output2.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "similarity = 1 - cosine(embedding1, embedding2)\n",
    "\n",
    "print(f\"Cosine similarity between the two sentences: {similarity:.4f}\") # 유사도 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AI_8/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222dfb16b77240b990e79ad4394834ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/298 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7026adc6c65c4f908edc7401d94bee77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/3.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ebf4e0ae2c432cb46f651b06334dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef7b476e3f0457d8233420d7d7b5513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3186f42bb0a44e7c97fe7a0ff6280ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/908 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac441cbc3754994bb9129bb359eac9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549ffa42b5e848de92ca0a03dd65ef69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AI_8/lib/python3.12/site-packages/transformers/generation/utils.py:1290: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: 미니 미니는 그런 꼬리\n"
     ]
    }
   ],
   "source": [
    "# M2M100 모델 : 번역해줌\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "\n",
    "# M2M100 모델과 토그나이저 로드\n",
    "model_name = \"facebook/m2m100_418M\"\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 번역할 문장\n",
    "sentence = \"minmin is such a cutie\"\n",
    "\n",
    "# 입력 문장 토큰화\n",
    "encoded_sentence = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# 번역 대상 언어 설정 (한국어로)\n",
    "tokenizer.src_lang = \"en\"\n",
    "model.config.forced_bos_token_id = tokenizer.get_lang_id(\"ko\")\n",
    "\n",
    "# 번역 수행\n",
    "generated_tokens = model.generate(**encoded_sentence)\n",
    "\n",
    "# 번역 결과를 디코딩\n",
    "translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Translated text: {translated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AI_8/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/AI_8/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594afc74a72644aaa71083a42e560cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'ko'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(sentence, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 번역 수행\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(inputs\u001b[38;5;241m.\u001b[39minput_ids, forced_bos_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mlang_code_to_id[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mko\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 번역 결과를 디코딩\u001b[39;00m\n\u001b[1;32m     19\u001b[0m translated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_tokens[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ko'"
     ]
    }
   ],
   "source": [
    "# NLLB-200 모델 : 번역해줌 ** 교안에 없고 강의에서도 정확히 안 나와서 모르겠음\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# NLLB-200 모델과 토그나이저 로드\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# 번역할 문장\n",
    "sentence = \"minmin is such a cutie\"\n",
    "\n",
    "# 번역 대상 언어 설정 (한국어로)\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# 번역 수행\n",
    "generated_tokens = model.generate(inputs.input_ids, forced_bos_token_id=tokenizer.lang_code_to_id['ko'])\n",
    "\n",
    "# 번역 결과를 디코딩\n",
    "translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Translated text: {translated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI_8)",
   "language": "python",
   "name": "ai_8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
