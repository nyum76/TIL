{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install scikit-learn` : 사이킷 런 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AI_8/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without fine-tuning: 0.5080\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 데이터셋 로드\n",
    "dataset = load_dataset(\"imdb\") # imdb : 영화의 리뷰 텍스트와 그에 대한 감정 레이블이 포함된 데이터셋. 영화 리뷰 감성 이진분류에 주로 사용\n",
    "# imdb 레이블 : 0 - 부정적 리뷰, 1 - 긍정적 리뷰\n",
    "\n",
    "# 테스트 데이터셋 설정\n",
    "test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(500))\n",
    " \n",
    "# 토크나이저로 입력 토큰화 진행\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# bert 모델이 이해할 수 있게 텍스트를 토큰형태로 변환\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], # imdb 데이터셋에서 리뷰를 가져옴\n",
    "        padding=\"max_length\", # 모든 텍스트를 최대길이로 패딩 - 모든 입력이 동일한 길이를 갖도록 함\n",
    "        truncation=True # 텍스트의 길이가 최대길이를 초과할 시 잘라냄\n",
    "    )\n",
    "\n",
    "# 데이터셋의 각 샘플에 토크나이즈펑션 진행. \n",
    "test_dataset = test_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True # 배치 단위로 함수를 적용 여러 샘플 한 번에 처리. 속도 높이고 메모리 최적화 됨\n",
    "    )\n",
    " \n",
    "# 데이터셋을 특정 형식으로 변환\n",
    "test_dataset.set_format(\n",
    "    type=\"torch\", # 파이토치 텐서로 변환\n",
    "    columns=['input_ids','attention_mask','label'] # 버트 모델에 필요한 데이터 컬럼만 선택\n",
    ")\n",
    "\n",
    "# Bert 모델 로드\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 2)\n",
    "\n",
    "# 모델을 평가 모드로 바꿈\n",
    "model.eval() \n",
    "\n",
    "# 예측 및 평가\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in torch.utils.data.DataLoader( # DataLoader : 데이터셋을 배치단위로 로드\n",
    "    test_dataset,  # 평가에 사용할 데이터셋\n",
    "    batch_size=8   # 배치 사이즈\n",
    "    ):\n",
    "    with torch.no_grad(): # 그래디언트를 계산하지 않음 -> 추론을 하기위해 사용\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask']) # 모델에 대한 아웃풋 생성\n",
    "    logits = outputs.logits \n",
    "    preds = np.argmax(logits.numpy(),axis=1)\n",
    "    all_preds.extend(preds)\n",
    "    all_labels.extend(batch['label'].numpy())\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = accuracy_score(all_labels, all_preds)    \n",
    "print(f\"Accuracy without fine-tuning: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파인튜닝\n",
    ": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDb 데이터셋 로드\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# 훈련 및 테스트 데이터셋 분리\n",
    "train_dataset = dataset['train'].shuffle(seed=42).select(range(1000))  # 1000개 샘플로 축소\n",
    "test_dataset = dataset['test'].shuffle(seed=42).select(range(500))  # 500개 샘플로 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/AI_8/lib/python3.12/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# BERT 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 데이터셋 토크나이징 함수 정의\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# 데이터셋 토크나이징 적용\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 모델 입력으로 사용하기 위해 데이터셋 포맷 설정\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Accelerator.__init__() got an unexpected keyword argument 'dispatch_batches'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m      6\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments( \u001b[38;5;66;03m# 모델훈련시 필요한 설정 정리\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# 훈련중 생성된 모델 파일 및 로그 저장 디렉토리\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;66;03m# 훈련 에포크 수\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     save_total_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;66;03m# 저장할 최대 체크포인트의 개수\u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 트레이너 설정\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer( \u001b[38;5;66;03m# 모델 훈련, 평가, 예측 간단히 수행할 수 있는 클래스\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, \u001b[38;5;66;03m# 훈련시킬 모델\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args, \u001b[38;5;66;03m# 학습을 위한 설정 값\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset, \n\u001b[1;32m     21\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 모델 훈련\u001b[39;00m\n\u001b[1;32m     25\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/AI_8/lib/python3.12/site-packages/transformers/trainer.py:347\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_accelerator_and_postprocess()\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker \u001b[38;5;241m=\u001b[39m TrainerMemoryTracker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mskip_memory_metrics)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/AI_8/lib/python3.12/site-packages/transformers/trainer.py:3969\u001b[0m, in \u001b[0;36mTrainer.create_accelerator_and_postprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3966\u001b[0m gradient_accumulation_plugin \u001b[38;5;241m=\u001b[39m GradientAccumulationPlugin(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgrad_acc_kwargs)\n\u001b[1;32m   3968\u001b[0m \u001b[38;5;66;03m# create accelerator object\u001b[39;00m\n\u001b[0;32m-> 3969\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;241m=\u001b[39m Accelerator(\n\u001b[1;32m   3970\u001b[0m     dispatch_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdispatch_batches,\n\u001b[1;32m   3971\u001b[0m     split_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msplit_batches,\n\u001b[1;32m   3972\u001b[0m     deepspeed_plugin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdeepspeed_plugin,\n\u001b[1;32m   3973\u001b[0m     gradient_accumulation_plugin\u001b[38;5;241m=\u001b[39mgradient_accumulation_plugin,\n\u001b[1;32m   3974\u001b[0m )\n\u001b[1;32m   3975\u001b[0m \u001b[38;5;66;03m# some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag\u001b[39;00m\n\u001b[1;32m   3976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather_for_metrics\n",
      "\u001b[0;31mTypeError\u001b[0m: Accelerator.__init__() got an unexpected keyword argument 'dispatch_batches'"
     ]
    }
   ],
   "source": [
    "# BERT 모델 로드\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "\n",
    "# 훈련 인자 설정\n",
    "training_args = TrainingArguments( # 모델훈련시 필요한 설정 정리\n",
    "    output_dir='./results', # 훈련중 생성된 모델 파일 및 로그 저장 디렉토리\n",
    "    num_train_epochs=3, # 훈련 에포크 수\n",
    "    per_device_train_batch_size=8, # 각 디바이스의 사용할 배치 크기 : 8개 샘플\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\", # 평가 전략 : 에포크가 끝날 때 마다 평가 진행\n",
    "    save_steps=10_000, # 10000 스텝마다 모델 저장\n",
    "    save_total_limit=2, # 저장할 최대 체크포인트의 개수\n",
    ")\n",
    "\n",
    "# 트레이너 설정\n",
    "trainer = Trainer( # 모델 훈련, 평가, 예측 간단히 수행할 수 있는 클래스\n",
    "    model=model, # 훈련시킬 모델\n",
    "    args=training_args, # 학습을 위한 설정 값\n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# 모델 훈련\n",
    "trainer.train()\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: acc}\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 이미 훈련된 트레이너에 compute_metrics를 추가하여 평가\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m trainer\u001b[38;5;241m.\u001b[39mcompute_metrics \u001b[38;5;241m=\u001b[39m compute_metrics\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 모델 평가 및 정확도 확인\u001b[39;00m\n\u001b[1;32m     15\u001b[0m eval_result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 평가 지표 함수 정의\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)  # 예측된 클래스\n",
    "    labels = p.label_ids  # 실제 레이블\n",
    "    acc = accuracy_score(labels, preds)  # 정확도 계산\n",
    "    return {'accuracy': acc}\n",
    "\n",
    "# 이미 훈련된 트레이너에 compute_metrics를 추가하여 평가\n",
    "trainer.compute_metrics = compute_metrics\n",
    "\n",
    "# 모델 평가 및 정확도 확인\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"Accuracy: {eval_result['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI_8)",
   "language": "python",
   "name": "ai_8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
