# Week2
## ANN
(Artificial Neural Network)
* 구성요소
  * 입력층 : 다양한 종류의 데이터 처리 (사진, table, 문자열 등)
  * 은닉층 :  (특징, 패턴, 규칙 등을 학습) 연산 수행
  * 출력층 : 다양한 문제 처리
    * 회귀 문제 (Regression)
      - 출력 레이어의 뉴런 수는 예측하려는 연속적인 값의 차원과 동일합니다.
      - 활성화 함수로는 주로 선형 함수(linear function)를 사용합니다.
    * 이진 분류 문제 (Binary Classification)
      - 출력 레이어의 뉴런 수는 1입니다.
      - 활성화 함수로는 시그모이드 함수(Sigmoid Function)를 사용하여 출력 값을 0과 1 사이의 확률로 변환합니다.
    * 다중 클래스 분류 문제 (Multi-Class Classification)
      - 출력 레이어의 뉴런 수는 예측하려는 클래스 수와 동일합니다.
      - 활성화 함수로는 소프트맥스 함수(Softmax Function)를 사용하여 각 클래스에 대한 확률을 출력합니다.
* 동작 방식
  * 순전파 : 입력층->은닉층->출력층
  * 손실함수 : 예측 값과 실제 값의 차이 (오차) 계산
  * 역전파 : 오차를 줄이기 위해 가중치 업데이트 (오차가 발생한 출력층에서부터 입력층으로)
    * 학습에서 hyperparameter 를 사용해 성능 조절 가능 (지역 최적값, 전역최적값)
    * 파라미터 : 업데이트를 하기 위한 가중치
* 실습
```py
# 실습을 위한 필수 라이브러리 불러오기
import torch # 핵심 라이브러리
import torch.nn as nn # 신경망 구축을 위한 라이브러리
import torch.optim as optim # 최적화 (함수를 최소 또는 최대로 맞추는 변수를 찾는 것)
import torchvision # 이미지 처리(입력)
import torchvision.transforms as transforms # 전처리

# 데이터셋 로드 및 전처리

# 데이터셋 전처리
transform = transforms.Compose([
    transforms.ToTensor(), # 이미지를 파이토치 기본 자료구조인 Tensor 로 바꿈
    transforms.Normalize((0.5,), (0.5,)) # 이미지를 정규화 (평균=0.5,표준편차=0.5)
])

# MNIST 데이터셋 로드
# (root : 다운 경로, train : 트레이닝 데이터셋인지, download : 다운로드 여부, transform : 전처리한 데이터를 다운)
trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
# (trainset : 트레인 데이터셋으로 데이터로더 만듬, batch_size : 데이터를 잘게 쪼개는 정도(쪼개면 학습 속도 빨라짐), shuffle : 데이터를 섞어 의존성 낮춤(독립성 보존))
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)

# ANN 모델 구현
# nn.Linear : ANN 모델 만드는 함수
class SimpleANN(nn.Module): # nn.Module : nn 모듈 상속받음(이미 있는 클래스의 기능을 새 클래스로 가져옴)
    def __init__(self):
        super(SimpleANN, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128)  # 입력층에서 은닉층으로 (28*28 : MNIST 데이터 크기)
        self.fc2 = nn.Linear(128, 64)       # 은닉층에서 은닉층으로
        self.fc3 = nn.Linear(64, 10)        # 은닉층에서 출력층으로 (10 : 0~9 예측을 위한 10개 퍼셉트론)
# fc (fully connected) 레이어 : 여러 개의 레이어가 서로 다 연결됨 (1차원 데이터를 받음)

    def forward(self, x): # 레이어 간의 연결관계 설정 (x : 입력 인자)
        x = x.view(-1, 28 * 28)  # 입력 이미지를 1차원 벡터로 변환 (-1 : 뒤의 차원을 바탕으로 )
        x = torch.relu(self.fc1(x)) # relu(활성화 함수) 함수로 입력 레이어에 x를 전달한 값을 다시 x에 저장
        x = torch.relu(self.fc2(x))
        x = self.fc3(x) # 최종 출력 레이어를 통과한 x 값이 나옴
        return x


# 모델 학습

# 모델 초기화
model = SimpleANN()

# 손실 함수와 최적화 알고리즘 정의
criterion = nn.CrossEntropyLoss() # crossentropyloss 손실함수 사용 : 분류모델성능평가
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) # SGD : 손실함수에 미적분을 해서 가중치를 올림. (lr : 학습률, momentum : 모멘텀 값)

# 모델 학습
for epoch in range(10):  # 10 에포크(전체 데이터를 몇 번 반복해서 보는지) 동안 학습
    running_loss = 0.0  # 어느정도로 손실함수가 발생했는지 출력
    for i, data in enumerate(trainloader, 0): # 데이터 로더에서 i(index) 와 데이터를 뽑음
        inputs, labels = data # input 과 lable 을 나눠줌

        # 기울기 초기화
        optimizer.zero_grad() # optimizer 가 연쇄법칙을 사용하므로 초기화해줘야 함

        # 순전파 + 역전파 + 최적화
        outputs = model(inputs) # 순전파 계산
        loss = criterion(outputs, labels) # loss 계산
        loss.backward() # 역전파를 통해 기울기 계산
        optimizer.step() # 기울기를 바탕으로 가중치 업데이트

        # 손실 출력
        running_loss += loss.item()
        if i % 100 == 99:  # 매 100 미니배치마다 출력
            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')
            running_loss = 0.0

print('Finished Training')


# 모델 평가
correct = 0
total = 0
with torch.no_grad(): # no_grad : 평가 단계에서는 기울기를 계산할 필요가 없으므로, 메모리 효율을 위해 사용
    for data in testloader: # 데이터로더에서 데이터를 하나씩 꺼냄
        images, labels = data # 데이터의 이미지와 레이블을 봄
        outputs = model(images) # 모델에 전달해 예측값을 봄
        _, predicted = torch.max(outputs.data, 1) # max 함수로 가장 가능성이 높은 퍼셉트론을 봄
        total += labels.size(0) # 레이블의 사이즈를 토탈에서 더해줌
        correct += (predicted == labels).sum().item() # 예측 값과 실제 값이 일치하는 샘플을 찾음

print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%') 




```







## CNN

## RNN