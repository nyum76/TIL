# Week2
## 인공 신경망, ANN
(Artificial Neural Network) : 사람의 신경망 모델링 -> 추론
* 구성요소
  * 입력층 : 다양한 종류의 데이터 처리 (사진, table, 문자열 등)
  * 은닉층 :  (특징, 패턴, 규칙 등을 학습) 연산 수행
  * 출력층 : 다양한 문제 처리
    * 회귀 문제 (Regression)
      - 출력 레이어의 뉴런 수는 예측하려는 연속적인 값의 차원과 동일합니다.
      - 활성화 함수로는 주로 선형 함수(linear function)를 사용합니다.
    * 이진 분류 문제 (Binary Classification)
      - 출력 레이어의 뉴런 수는 1입니다.
      - 활성화 함수로는 시그모이드 함수(Sigmoid Function)를 사용하여 출력 값을 0과 1 사이의 확률로 변환합니다.
    * 다중 클래스 분류 문제 (Multi-Class Classification)
      - 출력 레이어의 뉴런 수는 예측하려는 클래스 수와 동일합니다.
      - 활성화 함수로는 소프트맥스 함수(Softmax Function)를 사용하여 각 클래스에 대한 확률을 출력합니다.
* 동작 방식
  * 순전파 : 입력층->은닉층->출력층
  * 손실함수 : 예측 값과 실제 값의 차이 (오차) 계산
  * 역전파 : 오차를 줄이기 위해 가중치 업데이트 (오차가 발생한 출력층에서부터 입력층으로)
    * 학습에서 hyperparameter 를 사용해 성능 조절 가능 (지역 최적값, 전역최적값)
    * 파라미터 : 업데이트를 하기 위한 가중치
* 실습
```py
# 실습을 위한 필수 라이브러리 불러오기
import torch # 핵심 라이브러리
import torch.nn as nn # 신경망 구축을 위한 라이브러리
import torch.optim as optim # 최적화 (함수를 최소 또는 최대로 맞추는 변수를 찾는 것)
import torchvision # 이미지 처리(입력)
import torchvision.transforms as transforms # 전처리

# 데이터셋 로드 및 전처리

# 데이터셋 전처리
transform = transforms.Compose([
    transforms.ToTensor(), # 이미지를 파이토치 기본 자료구조인 Tensor 로 바꿈
    transforms.Normalize((0.5,), (0.5,)) # 이미지를 정규화 (평균=0.5,표준편차=0.5)
])

# MNIST 데이터셋 로드
# (root : 다운 경로, train : 트레이닝 데이터셋인지, download : 다운로드 여부, transform : 전처리한 데이터를 다운)
trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
# (trainset : 트레인 데이터셋으로 데이터로더 만듬, batch_size : 데이터를 잘게 쪼개는 정도(쪼개면 학습 속도 빨라짐), shuffle : 데이터를 섞어 의존성 낮춤(독립성 보존))
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)

# ANN 모델 구현
# nn.Linear : ANN 모델 만드는 함수
class SimpleANN(nn.Module): # nn.Module : nn 모듈 상속받음(이미 있는 클래스의 기능을 새 클래스로 가져옴)
    def __init__(self):
        super(SimpleANN, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128)  # 입력층에서 은닉층으로 (28*28 : MNIST 데이터 크기)
        self.fc2 = nn.Linear(128, 64)       # 은닉층에서 은닉층으로
        self.fc3 = nn.Linear(64, 10)        # 은닉층에서 출력층으로 (10 : 0~9 예측을 위한 10개 퍼셉트론)
# fc (fully connected) 레이어 : 여러 개의 레이어가 서로 다 연결됨 (1차원 데이터를 받음)

    def forward(self, x): # 레이어 간의 연결관계 설정 (x : 입력 인자)
        x = x.view(-1, 28 * 28)  # 입력 이미지를 1차원 벡터로 변환 (-1 : 뒤의 차원을 바탕으로 )
        x = torch.relu(self.fc1(x)) # relu(활성화 함수) 함수로 입력 레이어에 x를 전달한 값을 다시 x에 저장
        x = torch.relu(self.fc2(x))
        x = self.fc3(x) # 최종 출력 레이어를 통과한 x 값이 나옴
        return x


# 모델 학습

# 모델 초기화
model = SimpleANN()

# 손실 함수와 최적화 알고리즘 정의
criterion = nn.CrossEntropyLoss() # crossentropyloss 손실함수 사용 : 분류모델성능평가
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) # SGD : 손실함수에 미적분을 해서 가중치를 올림. (lr : 학습률, momentum : 모멘텀 값)

# 모델 학습
for epoch in range(10):  # 10 에포크(전체 데이터를 몇 번 반복해서 보는지) 동안 학습
    running_loss = 0.0  # 어느정도로 손실함수가 발생했는지 출력
    for i, data in enumerate(trainloader, 0): # 데이터 로더에서 i(index) 와 데이터를 뽑음
        inputs, labels = data # input 과 lable 을 나눠줌

        # 기울기 초기화
        optimizer.zero_grad() # optimizer 가 연쇄법칙을 사용하므로 초기화해줘야 함

        # 순전파 + 역전파 + 최적화
        outputs = model(inputs) # 순전파 계산
        loss = criterion(outputs, labels) # loss 계산
        loss.backward() # 역전파를 통해 기울기 계산
        optimizer.step() # 기울기를 바탕으로 가중치 업데이트

        # 손실 출력
        running_loss += loss.item()
        if i % 100 == 99:  # 매 100 미니배치마다 출력
            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')
            running_loss = 0.0

print('Finished Training')


# 모델 평가
correct = 0
total = 0
with torch.no_grad(): # no_grad : 평가 단계에서는 기울기를 계산할 필요가 없으므로, 메모리 효율을 위해 사용
    for data in testloader: # 데이터로더에서 데이터를 하나씩 꺼냄
        images, labels = data # 데이터의 이미지와 레이블을 봄
        outputs = model(images) # 모델에 전달해 예측값을 봄
        _, predicted = torch.max(outputs.data, 1) # max 함수로 가장 가능성이 높은 퍼셉트론을 봄
        total += labels.size(0) # 레이블의 사이즈를 토탈에서 더해줌
        correct += (predicted == labels).sum().item() # 예측 값과 실제 값이 일치하는 샘플을 찾음

print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%') 
```
## 합성곱 신경망, CNN
(Convolutional Neural Network) 사람의 시각 정보를 모델링 ->이미지
* 구성 요소
  * 합성곱
    * 합성곱 연산 : 필터와 이미지의 픽셀을 곱해 특징 맵을 생성
    * 필터 : 특징 학습
  * 풀링 : 특징 맵의 크기 줄임
    * Max Pooling : 필터 내 최대값 선택. 중요한 특징 강조, 불필요한 정보 제거
    * Average Pooling : 필터 내 평균값 계산. 특징 맵 크기 줄이면서 정보 손실 최소화
    *  풀링 외의 것
       *  padding : 이미지의 끝에 값을 줌(이미지 크기를 키움). CNN의 레이어가 깊어져도 이미지가 너무 작아지는 상황 방지
       *  stride : 필터 움직이는 정도
  * 완전 연결 : CNN은 이미지 처리에 특화, 출력엔 적합하지 않음->이후에 출력레이어로 ANN 활용

- 실습
```py
# 필요 라이브러리 불러오기
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms

 # 데이터셋 전처리
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# CIFAR-10 데이터셋 로드 : 배, 비행기 등의 이미지
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)  # 입력 채널(1=흑백, 3=RGB) 3, 출력 채널 32, 커널 크기 3x3 인 CNN모델 만들기
        self.pool = nn.MaxPool2d(2, 2)               # 풀링 크기 2x2
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # 입력 채널 32, 출력 채널 64, 커널 크기 3x3
        self.fc1 = nn.Linear(64 * 8 * 8, 512)        # 완전 연결 층 : ANN을 사용?
        self.fc2 = nn.Linear(512, 10)                # 출력 층 (10개의 클래스) : ANN 을 사용

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x))) # conv 뒤에도 활성화함수(relu) 있어야 함. 딥러닝의 기본가정
        x = self.pool(torch.relu(self.conv2(x)))
        x = x.view(-1, 64 * 8 * 8)  # 플래튼 : 2,3 차원 데이터의 차원을 펴 줌 (ANN과 CNN 을 연결하는 부분은 차원이 바뀜)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# 차원 살펴보기
for i data in enumerate(trainloader,0):
  print(inputs.shape) # 출력 : torch.Size([배치크기],[채널크기],[가로폭],[세로폭])


# 모델 초기화
model = SimpleCNN()

# 손실 함수와 최적화 알고리즘 정의
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

# 모델 학습
for epoch in range(10):  # 10 에포크 동안 학습
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data

        # 기울기 초기화
        optimizer.zero_grad()

        # 순전파 + 역전파 + 최적화
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # 손실 출력
        running_loss += loss.item()
        if i % 100 == 99:  # 매 100 미니배치마다 출력
            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')
            running_loss = 0.0

print('Finished Training')

# 모델 평가
correct = 0
total = 0
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')

```
## 순환 신경망, RNN
(Recurrent Neural Network) : 시퀀스 데이터(주가, 문장 등)를 다루는데 최적화
데이터가 순환함
t1시점에서 입력 받은 데이터의 출력을 다음 t2시점의 입력 데이터로 사용
순전파
  입력데이터가 순차적으로 네트워크 통과하여 출력 생성 
  각 시점에서 입력데이터와 이전 숨겨진 상태를 결합하여 새로운 숨겨진 상태를 개선하고 이를 출력을 만드는데 사용
역전파
  출력을 실제 값과 비교 -> 오류를 네트워크를 거슬러 올라가며 가중치 업데이트
  *모든 시점의 가중치가 동시에 업데이트됨!*
* 구조
  * 입력데이터와 이전 시간 단계의 은닉상태(누적)를 입력으로 받아서 현재 시간 단계의 은닉상태 출력
  * 은닉 상태를 시퀀스의 정보를 저장하고 다음 시간 단계로 전달
* 동작 원리
  * 시퀀스의 각 시간 단계에서 동일한 가중치 공유->시퀀스 패턴 학습
* 문제점
  * 오래된 기억들은 소실됨 (오래된 시간순으로)
  * 기울기 소실문제 : 학습이 계속 진행되어 층이 많이 쌓이면서 학습 정보가 점점 희미해짐. 학습이 잘 진행되지 않음
  * 해결
    * LSTM : gate 를 통해 cell 에 어떤 정보를 얼마나 저장하고 삭제할지 정함
      * 장점 : 긴 시퀀스에서도 정보를 잘 저장. 기울기 소실 문제 완화
      * 단점 : 복잡. 학습이 어려움. 학습이 오래걸림. 데이터 의존성 높음
    * GRU : **업데이트 게이트**가 새 정보를 얼마나 상태에 반영시킬지 정함. LSTM에서 더 발전해 셀 상태와 은닉 상태 통합한 버전. 이전 정보를 얼마나 무시할지 **리셋게이트**가 정함. 