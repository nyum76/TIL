# Week5
## 오토인코더
(Autoencoder) : 데이터의 중요한 특징을 학습하고 이를 복원하는 비지도 학습
차원축소, 잡음제거, 생성 모델 등
* 구조
  * 인코더 : 인풋을 저차원 공간으로 표현( 중요 특징만 추출 )
    * 잠재 공간 (Latent Space) : 높은 차원의 입력 데이터를 저차원으로 바꾼 공간 (공간이 압축됨)
  * 디코더 : 추출된 정보를 바탕으로 원본 데이터를 만듬
* 종류
  * 딥 오토인코더 : 딥러닝에서의 오토인코더
  * 변분 오토인코더 (VAE) : 데이터의 잠재 공간을 사용해 데이터 분포 학습
  * 희소 오토인코더 : 출력이 희소하게 하여 중요 특징만 학습
  * 잡음 제거 오토인코더 : 입력 데이터에 잡음 추가, 이를 제거하는 학습을 통해 데이터 복원 능력 향상


## 생성형 모델 학습
### GAN
* 구성요소
  * 생성자 : 랜덤 데이터를 받아 실제 이미지처럼 가짜 이미지를 만들어내도록 학습
  * 판별자 : 생성된 이미지가 진짜인지 가짜인지 판별
* 핵심 원리
  * 두 개의 모듈이 서로 경쟁 하며 학습
    * 생성자는 판별자를 속이기 위해 가짜이미지를 만들고, 판별자는 진위여부를 잘 판단하기위해 발전함
* 장점
  * 고품질 데이터 생성
  * 다양한 작업 가능
* 단점
  * 훈련이 불안정함 : 경쟁이 잘 조율되게 만들기 쉽지 않음


## VAE
(Variational Autoencoder)
입력 데이터를 잠재 공간에 확률 분포로 모델링 해서 새로운 데이터 생성 후 해당 데이터의 구조적 특징 학습하는 모델
* 구조
  * 인코더 : 입력 데이터를 잠재 변수(확률분포를 따름)로 변환
  * 디코더 : 잠재 변수로 부터 원래 데이터를 복원
  * 손실함수
    * 리컨트럭션 로스 : 원래 데이터와 복원된 데이터간의 차이를 최소화
    * 쿨백라이블러 발산? : 인코더가 학습한 잠재 분포와 정규분포간의 차이를 최소화
* 장점 
  * 새로운 데이터를 생성 
* 단점
  * 복잡해서 훈련이 어려움

## 전이 학습
(Transfer Learning)
이미 학습된 모델의 지식을 새로운 문제에 적용해 새로운 문제를 더 빠르고 효과적으로 해결
* 장점
  * 데이터 부족할 때 기존 모델의 지식을 활용해 해결 가능
  * 사전 학습된 모델 사용시 **학습 시간 단축**됨
  * 성능 향상
* 원리
  * 특징 추출기 : 사전 학습된 모델의 초기 층 고정, 새 데이터에 맞게 **마지막 층만 재학습**
  * 미세 조정 (Fine-Tuning) : 사전 학습 **모델 전체**를 새로운 데이터에 맞게 재학습(가중치 바꾸기)
* 과정
  * 사전 학습 모델 로드 : PyTorch 에서 제공하는 사전 학습된 모델 로드
  * 모델 수정 : 사전 학습된 모델의 마지막 층을 새 문제에 맞게 수정
  * 모델 학습 : 수정된 모델 새 데이터에 맞게 학습