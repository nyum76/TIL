# Week6
## 과적합 방지 기법
과적합 : 모델이 학습데이터에 지나치게 의존해서 일반화가 안되는 문제
### 정규화
(Normalization) : 데이터를 일정 범위로 조정
* 종류
  * 배치 정규화 : 미니배치에서 특징의 평균과 분산을 사용해 활성화 값을 정규화->학습 속도 향상. 초기화 값에 민감하지 않아도 됨. 과적합 방지
    * 미니배치 : 전체 데이터셋을 작은 부분 집합으로 나눈 것
    * 활성화 값 : 활성화 함수를 통해 계산됨
  * 레이어 정규화 : 각 레이어의 활성화 값을 정규화. 배치크기에 영향을 받지 않고 시퀀스 데이터에 적합해 RNN에 주로 쓰임
  * 그룹 정규화 : 채널을 그룹으로 나누고 각 그룹에서 정규화
  * 인스턴스 정규화 : 각 샘플에 대해 정규화

### 드롭아웃
학습 과정에서 무작위로 뉴런을 비활성화해 과적합 방지
* **학습 시에만 적용!!!** 평가 시에는 모든 뉴런을 활성화 해야 함

### 조기 종료
* 검증 데이터의 성능이 더 이상 향상되지 않을 때 학습 중단

### 데이터 증강
원본 데이터를 변형해 새 데이터 생성하여 데이터셋 확장해 모델의 일반화 성능 향상

## 하이퍼파라미터 튜닝
최적화 방법은 모델의 가중치를 업데이트 해서 손실함수를 최소화 하는 알고리즘임
* 용어
  * 학습률
    * 어느정도의 비율로 학습할 지 정함
    * 너무 큼- > 학습 불안정 / 너무 작음 -> 학습 느려짐 
    * 일반적으로 0.1,0.01,0.001 등의 값 사용
  * 배치 크기
    * 한 번의 업데이트에 사용되는 데이터의 샘플 수 결정
    * 클수록 학습 속도는 증가하지만 메모리 사용량도 증가
    * 일반적으로 32,64,128 
  * 에포크 수
    * 전체 데이터셋 학습 반복 횟수
    * 너무 적으면 과소적합, 너무 크면 과적합 발생
    * 조기 종료 기법을 사용해 적절한 에포크 수를 결정 가능
  * 모멘텀
    * 경사하강법 : 손실함수를 최소화하기 위해 모델의 가중치를 반복적 업데이트
    * 이전 기울기를 현재 기울기에 반영해, 학습 속도를 높이고 진동을 줄임
    * 일반적으로 0.9, 0.99 값
  * 가중치 초기화
    * 모델의 가중치를 초기화 하는 법 결정
## 모델 평가와 검증 및 PyTorch 문법 정리
* K-Fold 교차검증
  * 데이터를 K개의 폴드로 나누고 각 폴드에 대해 학습과 검증 수행
* PyTorch 문법 정리
  * `torch.nn.Module`: 기본 모델 구축
  ```py
    import torch.nn as nn

    class MyModel(nn.Module): # MyModel
        def __init__(self): # 구조 만들기
            super(MyModel, self).__init__()
            self.layer1 = nn.Linear(10, 20)

        def forward(self, x): # 데이터가 어떻게 흘러갈 지 봄
            x = self.layer1(x)
            return x
  ```